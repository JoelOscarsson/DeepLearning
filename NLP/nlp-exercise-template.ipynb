{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c19d029-25f5-4d8e-9514-bf3ad9d30026",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-13 18:54:56.842851: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/joeloscarsson/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN, Dense\n",
    "\n",
    "import ssl \n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88ce5d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords in NLTK:  179\n"
     ]
    }
   ],
   "source": [
    "\n",
    "stopwords_default = stopwords.words('english')\n",
    "print('Stopwords in NLTK: ',len(stopwords_default))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e2401a-3b28-4a42-ad99-4623e795c5b3",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5766eb4-ca56-44be-9c47-23a102b3d3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/imdb_train_data_small.csv')\n",
    "test_df = pd.read_csv('../data/imdb_test_data_small.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6342a5a9-2277-422f-96af-351bad324de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This movie has bad writing and bad editing. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I'm still laughing- Not! I'm still asking my m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>While I'm normally a big fan of John Turturro'...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;br /&gt;&lt;br /&gt;The author tried to make a Kevin S...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Oh boy ! It was just a dream ! What a great id...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295</th>\n",
       "      <td>My wife and I struggle to find movies like thi...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>296</th>\n",
       "      <td>While watching this film recently, I constantl...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>297</th>\n",
       "      <td>Trust the excellent and accurate Junagadh75 re...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>Valley Girl is an exceptionally well made film...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>\"Just before dawn \" is one of the best slasher...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    This movie has bad writing and bad editing. It...      0\n",
       "1    I'm still laughing- Not! I'm still asking my m...      0\n",
       "2    While I'm normally a big fan of John Turturro'...      0\n",
       "3    <br /><br />The author tried to make a Kevin S...      0\n",
       "4    Oh boy ! It was just a dream ! What a great id...      0\n",
       "..                                                 ...    ...\n",
       "295  My wife and I struggle to find movies like thi...      1\n",
       "296  While watching this film recently, I constantl...      1\n",
       "297  Trust the excellent and accurate Junagadh75 re...      1\n",
       "298  Valley Girl is an exceptionally well made film...      1\n",
       "299  \"Just before dawn \" is one of the best slasher...      1\n",
       "\n",
       "[300 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693a8840-336c-4b42-b6b8-2c4cc53f229f",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "Create your own tokenization algorithm. Remember to handle upper/lower case, comma, punctioation and so on.\n",
    "Each word should hava an integer connected to it. Word as key and integer as value in a dict is one way to do it.\n",
    "\n",
    "Tensorflow have tokenization models, but try to bild it yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e6b2cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>has</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>editing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index tokenized_text\n",
       "0      0           this\n",
       "1      0          movie\n",
       "2      0            has\n",
       "3      0            bad\n",
       "4      0        writing\n",
       "5      0            and\n",
       "6      0            bad\n",
       "7      0        editing\n",
       "8      0             it\n",
       "9      0             is"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "        13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "        26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "        39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "        52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "        65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "        78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "        91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "       104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "       117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "       130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "       143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "       156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "       169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "       182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "       195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "       208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "       221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "       234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "       247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "       260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "       273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "       286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "       299])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_df = pd.read_csv('../data/imdb_test_data_small.csv')\n",
    "\n",
    "test_df\n",
    "def tokenize(test_df): #-> int: \n",
    "    \"\"\" Tokenize the text column in the dataframe,\n",
    "    returns a new column with the tokenized words\"\"\"\n",
    "    \n",
    "    test_df['text'] = test_df['text'].apply(lambda text: ''.join(c.lower() for c in text if c.isalpha() or c.isspace()).strip())\n",
    "\n",
    "    text = test_df['text']\n",
    "\n",
    "    test_df['tokenized_text'] = test_df['text'].apply(lambda row: nltk.word_tokenize(row))\n",
    "\n",
    "    return test_df\n",
    "\n",
    "\n",
    "text = tokenize(test_df)\n",
    "new_df = pd.DataFrame({'tokenized_text': text['tokenized_text']})\n",
    "\n",
    "tokenized_text = new_df['tokenized_text'].explode().reset_index()\n",
    "\n",
    "\n",
    "# print(tokenized_text.head())\n",
    "\n",
    "display(tokenized_text.head(10))\n",
    "display(tokenized_text['index'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2566c0-015a-4cdf-b88d-b41376df220d",
   "metadata": {},
   "source": [
    "# Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91ff2b32-2e36-4848-8be9-c7e880d3ddac",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "#stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbb2b9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>editing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>difficult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>follow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>characters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>makes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>sense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>calamity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>occurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>seconds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>sequences</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>hours</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>throw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>scenes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index         word\n",
       "1       0        movie\n",
       "3       0          bad\n",
       "4       0      writing\n",
       "6       0          bad\n",
       "7       0      editing\n",
       "10      0    difficult\n",
       "12      0       follow\n",
       "15      0        going\n",
       "18      0      nothing\n",
       "22      0   characters\n",
       "24      0        makes\n",
       "25      0         much\n",
       "26      0        sense\n",
       "28      0        major\n",
       "29      0     calamity\n",
       "30      0       occurs\n",
       "31      0        every\n",
       "32      0      seconds\n",
       "35      0       result\n",
       "36      0         none\n",
       "39      0       action\n",
       "40      0    sequences\n",
       "44      0  interesting\n",
       "46      0        movie\n",
       "48      0          two\n",
       "49      0        hours\n",
       "50      0        worth\n",
       "52      0        throw\n",
       "53      0         away\n",
       "54      0       scenes"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_stopwords(tokenized_text, stop_words):\n",
    "    \"\"\" Remove stopwords from the tokenized text column\"\"\"\n",
    "\n",
    "    tokenized_text['tokenized_text'] = tokenized_text['tokenized_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "\n",
    "tokenized_text = remove_stopwords(tokenized_text.copy(), stop_words)\n",
    "\n",
    "# Display the new column\n",
    "tokenized_text = tokenized_text[tokenized_text['tokenized_text'].apply(len) > 0]\n",
    "# rename column\n",
    "tokenized_text = tokenized_text.rename(columns={'tokenized_text': 'word'})\n",
    "tokenized_text.head(14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ae56ad-156e-4a0d-be28-d17914bcc475",
   "metadata": {},
   "source": [
    "# Lemmatization\n",
    "\n",
    "Is about reducing words to their base form. \n",
    "\n",
    "* \"running\" → \"run\"\n",
    "* \"better\" → \"good\"\n",
    "* \"mice\" → \"mouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77df94d-37e1-4615-a8cf-051296763a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73d33ae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>writing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>editing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>difficult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>follow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>going</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>nothing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>character</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>make</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>much</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>sense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>major</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>calamity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>occurs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>every</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>second</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0</td>\n",
       "      <td>result</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0</td>\n",
       "      <td>none</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0</td>\n",
       "      <td>action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0</td>\n",
       "      <td>sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0</td>\n",
       "      <td>interesting</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0</td>\n",
       "      <td>movie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0</td>\n",
       "      <td>two</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0</td>\n",
       "      <td>hour</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0</td>\n",
       "      <td>worth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0</td>\n",
       "      <td>throw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0</td>\n",
       "      <td>away</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0</td>\n",
       "      <td>scene</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index         word\n",
       "1       0        movie\n",
       "3       0          bad\n",
       "4       0      writing\n",
       "6       0          bad\n",
       "7       0      editing\n",
       "10      0    difficult\n",
       "12      0       follow\n",
       "15      0        going\n",
       "18      0      nothing\n",
       "22      0    character\n",
       "24      0         make\n",
       "25      0         much\n",
       "26      0        sense\n",
       "28      0        major\n",
       "29      0     calamity\n",
       "30      0       occurs\n",
       "31      0        every\n",
       "32      0       second\n",
       "35      0       result\n",
       "36      0         none\n",
       "39      0       action\n",
       "40      0     sequence\n",
       "44      0  interesting\n",
       "46      0        movie\n",
       "48      0          two\n",
       "49      0         hour\n",
       "50      0        worth\n",
       "52      0        throw\n",
       "53      0         away\n",
       "54      0        scene"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def lemmatize(lemmatizer, tokenized_text):\n",
    "    \"\"\"Lemmatize the tokenized 'word' column\"\"\"\n",
    "\n",
    "    # Function to lemmatize a single word\n",
    "    def lemmatize_word(word):\n",
    "        return lemmatizer.lemmatize(word)\n",
    "\n",
    "\n",
    "    tokenized_text['word'] = tokenized_text['word'].apply(lemmatize_word)\n",
    "\n",
    "    return tokenized_text\n",
    "\n",
    "lemmatized_df = lemmatize(lemmatizer, tokenized_text)\n",
    "lemmatized_df.head(14)  # We can see that some words are lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab300587",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9f0f1fe-e10c-4bc1-8533-b22f7b194f3b",
   "metadata": {},
   "source": [
    "# Word embedding and sentiment analysis model\n",
    "We want to create a model that can say if a movie review is bad or good.\n",
    "\n",
    "- Preprocess the text\n",
    "- Convert text to seqiuence of integers\n",
    "- Create architecture that includes embeddings\n",
    "- Build and train your models\n",
    "- Evaluate preformance\n",
    "\n",
    "Building models from scratch is not something you usually do, but those who would like to dig deeper into the math behind Simple RNN, LSTM and GRU can do it by creating the cells from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c67861b1-61e9-4b7f-a525-f27c11093338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_data(embedded_text):\n",
    "    # All sentences should be of the same lenght, but if a sentence is shorter than the longest, pad it.\n",
    "    return padded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3425e-214e-4f0d-89fa-dd61faacbda3",
   "metadata": {},
   "source": [
    "## RNN with tensorflow modules\n",
    "[Simple RNN cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/SimpleRNN)\n",
    "\n",
    "[Embedding](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac733a4c-b314-4b2e-ae0d-295e3e2671d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rnn_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01152d72-7062-4d00-b3f4-7d1ae3b30f3e",
   "metadata": {},
   "source": [
    "## RNN from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ba7e11b-131c-49b7-9a3c-0fcd363a632e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (177378052.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[14], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    self.Wxh =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class RNNCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.Wxh =\n",
    "        self.Whh =\n",
    "        self.bh =\n",
    "\n",
    "    def __call__(self, x, h):\n",
    "        h_next = \n",
    "        return h_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9177958f-cbe0-466c-b39e-78a87ce4f1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Model Class\n",
    "class MyRNNModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=1, sequence_length=100):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.rnn_cell = RNNCell(embedding_dim, hidden_dim)\n",
    "        self.Why = \n",
    "        self.by = \n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = \n",
    "        h = \n",
    "\n",
    "        # Process the input sequence\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.rnn_cell(x_t, h)\n",
    "\n",
    "        y = \n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5433047d-199f-4aab-858c-f4e25bba4550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ececbef-f1f6-4d36-97ee-42ce80b3ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11f54b-f41a-4f5a-b1f4-5c5390d9bb6a",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "[LSTM Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4902ef9-520c-4975-9aff-f89b623104e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198898ec-771e-4b89-b6dc-12944f0c6d19",
   "metadata": {},
   "source": [
    "## LSTM from scrtch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3a92a1-e930-41c5-8625-4b6c32adf328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Cell Class\n",
    "class LSTMCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Gates: input, forget, cell, output\n",
    "        self.Wi =\n",
    "        self.Wf =\n",
    "        self.Wc =\n",
    "        self.Wo =\n",
    "        self.bi =\n",
    "        self.bf =\n",
    "        self.bc =\n",
    "        self.bo =\n",
    "\n",
    "    def __call__(self, x, h, c):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        i = \n",
    "        f = \n",
    "        o = \n",
    "        c_ = \n",
    "\n",
    "        c_new = \n",
    "        h_new =\n",
    "\n",
    "        return h_new, c_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40f3545-5a8f-45d7-813e-075ae61dd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM Model Class\n",
    "class MyLSTMModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.lstm_cell = LSTMCell(embedding_dim, hidden_dim)\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "        c =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h, c = self.lstm_cell(x_t, h, c)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d248f5-0b90-42f7-99ce-351167d00b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    clipped_gradients = [tf.clip_by_norm(g, clip_norm) for g in gradients]\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9d6d6e-48d0-4b40-b402-f81b13b62c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a3810e-0ae8-4058-b749-799b254f3e81",
   "metadata": {},
   "source": [
    "## GRU\n",
    "[GRU Cell](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GRUCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0da681b6-0ae3-4459-87f4-c8184009ea8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gru_model():\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a06ce2-4f0e-4fb8-b42a-d307982da693",
   "metadata": {},
   "source": [
    "## GRU from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aae6a39-8185-4260-9d6b-e1bb45d2b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Cell Class\n",
    "class GRUCell(tf.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Update gate parameters\n",
    "        self.Wz =\n",
    "        self.bz =\n",
    "\n",
    "        # Reset gate parameters\n",
    "        self.Wr =\n",
    "        self.br =\n",
    "\n",
    "        # Candidate hidden state parameters\n",
    "        self.Wh =\n",
    "        self.bh =\n",
    "        \n",
    "    def __call__(self, x, h):\n",
    "        combined = tf.concat([x, h], 1)\n",
    "\n",
    "        # Update gate\n",
    "        z =\n",
    "\n",
    "        # Reset gate\n",
    "        r =\n",
    "\n",
    "        # Candidate hidden state\n",
    "        combined_reset =\n",
    "        h_candidate =\n",
    "\n",
    "        # New hidden state\n",
    "        h_new =\n",
    "\n",
    "        return h_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7b6bb3-7353-4aa4-ab88-bf008934661d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU Model Class\n",
    "class MyGRUModel(tf.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.embedding =\n",
    "        self.gru_cell =\n",
    "        self.Why =\n",
    "        self.by =\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x =\n",
    "        h =\n",
    "\n",
    "        for t in range(sequence_length):\n",
    "            x_t = x[:, t, :]\n",
    "            h = self.gru_cell(x_t, h)\n",
    "\n",
    "        y =\n",
    "        return tf.sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5653c0e5-4d7c-443a-9bce-566c92fcac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, inputs, targets):\n",
    "    clip_norm = 1.0\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs)\n",
    "        loss = loss_function(targets, predictions)\n",
    "        \n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(model.trainable_variables)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b3eae4-cda1-4574-bc05-43869b94fc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((padded_train_data, y)).batch(batch_size)\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0\n",
    "    epoch_accuracy = 0\n",
    "    total_batches = 0\n",
    "\n",
    "    for batch_inputs, batch_targets in train_dataset:\n",
    "        loss = train_step(model, batch_inputs, batch_targets)\n",
    "        epoch_loss += loss.numpy()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        predictions = model(batch_inputs)\n",
    "        accuracy = calculate_accuracy(batch_targets, predictions)\n",
    "        epoch_accuracy += accuracy.numpy()\n",
    "\n",
    "        total_batches += 1\n",
    "\n",
    "    avg_loss = epoch_loss / total_batches\n",
    "    avg_accuracy = epoch_accuracy / total_batches\n",
    "    print(f'Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
